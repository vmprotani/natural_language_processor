---
title: "Text Predictor Exploratory Analysis"
author: "Vince Protani"
date: "6/1/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo=FALSE, message=FALSE, warning=FALSE, result=FALSE}
library(data.table)
library(ggplot2)
library(ggpubr)
n <- 1:4
sources <- c("blogs", "news", "twitter")
num.sources <- length(sources)
```
## Introduction

This documnet explores the training data for the text prediction application using a modified Katz back-off model. The data consists of ngrams between 1 and 4 words derived from a corpora provided by Johns Hopkins University. The entire corpora was used to create the ngrams. The resulting files composed over 4 GB worth of data, so ngrams with a frequency of 1 are removed. The resulting used space is roughly 150 MB. 

All source code and references can be found in [this project's repository](https://github.com/vmprotani/text_predictor/).

## Creating Plots from Ngram Data

We look at the ngrams two ways: frequency from their respective sources (blogs, news, or twitter), and then all together. Only the top few ngrams are shown.

```{r}
sep.plots <- list()
tot.plots <- list()
for (II in n) {
  ngram.files <- paste0("../tidy_data/en_US.", sources, ".", II, "grams.txt")
	ngram.str <- paste0(II, "gram")
	
	ngrams <- lapply(ngram.files, fread, sep=",")
	if (II > 1) {
	  lapply(ngrams, function(x) {
	    x[,ngram:=paste(start,end,sep="_")]
	    x[,start:=NULL]
	    x[,end:=NULL]})
	} else {
	  lapply(ngrams, function(x) {
	    x[,ngram:=start]
	    x[,start:=NULL]})
	}
	top <- lapply(ngrams, function(x) x[1:5,])

	sep.subplots <- lapply(1:num.sources, function(x) {
	  ggplot(top[[x]], aes(x=reorder(ngram, count), y=count, fill=count)) + geom_col() + 
	    scale_y_continuous(labels=function(x) format(x, scientific=FALSE)) + coord_flip() + 
	    theme(legend.position="none") + labs(x=ngram.str, y="count", title=sources[x]) + 
	    xlab(NULL) + ylab(NULL)})
	sep.plots[[II]] <- ggarrange(sep.subplots[[1]], sep.subplots[[2]], sep.subplots[[3]],
	                             nrow=3)

	ngrams <- Reduce("rbind", ngrams)
	ngrams[,source:=NULL]
	ngrams <- ngrams[,.(count=sum(count)), by=ngram]
	setorder(ngrams, -count, ngram)
	ngrams <- ngrams[1:20,]

	tot.plots[[II]] <- ggplot(ngrams, aes(x=reorder(ngram, count), y=count, fill=count)) +
	  scale_y_continuous(labels=function(x) format(x, scientific=FALSE)) +
	  geom_bar(stat="identity") + coord_flip() + theme(legend.position="none") + xlab(NULL) +
	  ylab(NULL) + labs(title="all sources")
}
```

## Unigram Analysis

```{r echo=FALSE}
ggarrange(sep.plots[[1]], tot.plots[[1]], ncol=2)
```

## Bigram Analysis

```{r echo=FALSE}
ggarrange(sep.plots[[2]], tot.plots[[2]], ncol=2)
```

## Trigram Analysis

```{r echo=FALSE}
ggarrange(sep.plots[[3]], tot.plots[[3]], ncol=2)
```

## Quadrigram Analysis

```{r echo=FALSE}
ggarrange(sep.plots[[4]], tot.plots[[4]], ncol=2)
```

## Observations

Though it may be tempting to remove stop words from predictions, from the above graphs, we can see how frequent they are. Therefore, it would be inaccurate to ignore them when predicting words. As can be assumed from these graphs, numbers have been removed since they provide little to no help in predictions; "two weeks later" is just as probable as three, five, or ten weeks later. Including the restriction on ngrams occurring only one time, our datasets are concise and provide a snapshot of natural language to feed the algorithm.

## Planned Model

This application will implement a Katz back-off model. To find predictions, the user's input will be compared against the beginnings of ngrams and the most frequent endings to those endgrams will become predictions. For example, if the user enters "today has been such a great," the app will take "such a great" (given the order of ngrams is 4, which begin with 3 words) and compare that against the quadrigrams for matches. If none are found, "a great" will be searched for in the trigrams. This back-off is performed until a prediction is found.

Though this method can eliminate a lot of the context in sentences that make them make sense - i.e. subject-verb relationships can be ignored - it is an efficient way to search a large body of text for possible predictions.
