---
title: "Natural Language Processor Exploratory Analysis"
author: "Vince Protani"
date: "5/20/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r warning=FALSE, message=FALSE}
library(caret)
library(plyr)
library(dplyr)
library(tidytext)
library(ggpubr)
library(tidyr)
set.seed(20190521)
```

## Introduction

This document explores the training data for a natural language processing model. Below are the libraries needed for the analysis.

##  Exploratory Analysis

*(Data reading and ngram manipulation is not presented here to save space. The sourced code can be found in its respective file.)*

```{r}
source("read_tidy_data.R")
source("create_ngrams.R")
isWord <- function(x) { grepl("^[a-z\']+$", x) }
vowelRepeats <- function(x) { grepl("(a|i|u){2,}|(e|o){3,}", x) }
```

Our goal here is to predict text given three words. We inspect ngrams of two, three, and four words that could be used to predict a word given user input. In this case, the input would be mapped to an ngram with one more word than the input has. For example, if the input has three words, ngrams of four words would be searched for a prediction. However, it is helpful to look at ngrams of two and three to broaden our search for a prediction.

```{r}
ngram2 <- createNgrams(2)

stopFreq2 <- ngram2$count %>% group_by(word1) %>% 
  summarize(stop=sum(word2 %in% stop_words$word), reg=sum(!stop), 
            stopRatio=stop/(stop+reg)) %>% ungroup() %>%
  summarize(avg=mean(stopRatio))
regFreq2 <- 1-stopFreq2$avg

ngram2$fig

source("partition_ngrams_2.R")
```

We can see the most frequent pairs of two words are very common phrases, and they do not differ much between the sources. We can also see the frequency of the bigrams from blogs is significantly higher than from news and twitter, whch dominates the frequent phrases over all of the sources. 

```{r}
ngram3 <- createNgrams(3)

stopFreq3 <- ngram3$count %>% group_by(word1, word2) %>% 
  summarize(stop=sum(word3 %in% stop_words$word), reg=sum(!stop), 
            stopRatio=stop/(stop+reg)) %>% ungroup() %>%
  summarize(avg=mean(stopRatio))
regFreq3 <- 1-stopFreq3$avg

ngram3$fig

source("partition_ngrams_3.R")
```

The ngrams of three words involve slightly different phrases, but similarly to the pairs of two words, the final word in almost all phrases is a stop word. Unlike the ngrams of two words, the most frequent ngrams of three are numerically comparable between blogs and twitter while news falls behind.

```{r}
ngram4 <- createNgrams(4)

stopFreq4 <- ngram4$count %>% group_by(word1, word2, word3) %>% 
  summarize(stop=sum(word4 %in% stop_words$word), reg=sum(!stop), 
            stopRatio=stop/(stop+reg)) %>% ungroup() %>%
  summarize(avg=mean(stopRatio))
regFreq4 <- 1-stopFreq4$avg

ngram4$fig

source("partition_ngrams_4.R")
```

```{r echo=FALSE}
rm(data)
```

From first glance, there seems to be more phrases in the 4-word ngrams that end in a non-stop word. While these have a richer diversity of words, these pairings appear much less frequently than the 2- and 3-word ngrams.

```{r}
freqDat <- tibble(frequency=c(stopFreq2, regFreq2, stopFreq3, regFreq3, stopFreq4, regFreq4),
                  n=c("2", "2", "3", "3", "4", "4"), 
                  wordType=c("stop", "regular", "stop", "regular", "stop", "regular"))
ggplot() + geom_bar(aes(y=frequency, x=n, fill=wordType), data=freqDat, stat="identity") + labs(title="likelihood of ngram's final word classifications", fill="type of words", x="words in ngrams")
```

Overall, we can see that for any given ngram, it is much more probable that the last word is a stop word rather than a regular word. This intuitively makes sense because stop words are, by definition, the most common words.

## Planned Model

I plan to have my model accept a phrase, search the corresponding set of ngrams, and return the three most frequent last words of the ngram. For example, if the user inputs three words, the model will inspect the table of 4-word ngrams, collect all rows in which the first three words match the user's input, and return the three most common words that follow those.

I would also like to incorporate smaller ngram usage if the search for the original input fails. Continuing with the above example, if the three words the user submitted cannot be found, the last two words of that input will be searched in the 3-word ngrams data, and the three most common third words for that pair of words are returned. Ultimately, the purpose is to return a list of the three most probable words for the prediction by inspecting the frequency of words in ngrams involving the user's input.

Also, seeing that the overall frequency of phrases ending in stop words is high, I do not plan on differentiating ruling them out for the prediction model. It makes sense to predict using stop words, because otherwise the prediction would be ignoring the words most likely to be used. 

### Training and Test Sets

There are a few modifications to the sets of ngrams. Ngrams containing the following are removed:

* numbers, because they are much more variable than words;

* vowels repeated beyond what is normal for words are removed; and

* a word repeated back-to-back.

After all these cases are removed, for each set of ngrams, the most frequent cases are kept, and the rest are removed. Specifically, the three most frequent final words for any ngram are kept or all if there are less than 3; since the prediction app will only display three words, there do not need to be more than 3 options maintained for any ngram. These removals optimize the amount of memory used and remove foreseeable edge cases in predictions, and we are left with the following number of ngrams.

```{r}
sets <- tibble(n2=NROW(filtered2), n3=NROW(filtered3), n4=NROW(filtered4))
sets %>% mutate(total=rowSums(sets))
```

```{r}
write.table(filtered2, file="./ngrams/ngrams2.txt", sep="\t", row.names=FALSE, quote=FALSE)
write.table(filtered3, file="./ngrams/ngrams3.txt", sep="\t", row.names=FALSE, quote=FALSE)
write.table(filtered4, file="./ngrams/ngrams4.txt", sep="\t", row.names=FALSE, quote=FALSE)
```