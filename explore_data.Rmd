---
title: "Natural Language Processor Exploratory Analysis"
author: "Vince Protani"
date: "5/20/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r warning=FALSE, message=FALSE}
library(caret)
library(dplyr)
library(tidytext)
library(ggpubr)
```

```{r echo=FALSE}
set.seed(20190521)
```

## Introduction

This document explores the training data for a natural language processing model. Below are the libraries needed for the analysis.

## Pre-processing

The data has been processed into a tidy format organized by senteces per file. For each file, 25% of the lines are kept to preserve memory. Each line is then separated by sentences, and punctuation is removed. The script used to pre-process the data is read_data.R.

##  Train and Test Sets

```{r echo=FALSE}
dataNames <- c("blogs", "news", "twitter")
dataDir <- "./tidy_data/"
dataFileNames <- c("en_US.blogs.lines.txt", "en_US.news.lines.txt", "en_US.twitter.lines.txt")
dataFiles <- paste(dataDir, dataFileNames, sep="")
filesIndex <- 1:length(dataFiles)

data <- lapply(dataFiles, read.table, header=TRUE, sep="\t", fill=TRUE, quote="", stringsAsFactors=FALSE)
data <- lapply(data, function(x) tibble(sentence=x$sentence))
               
inTrain <- lapply(filesIndex, function(x) createDataPartition(1:NROW(data[[x]]), p=0.7, list=FALSE))
train <- lapply(filesIndex, function(x) as_tibble(data[[x]][inTrain[[x]], ]))
test <- lapply(filesIndex, function(x) as_tibble(data[[x]][-inTrain[[x]], ]))

numSentences <- tibble(set=c("train", "test"), blogs=c(NROW(train[[1]]), NROW(test[[1]])),
                       news=c(NROW(train[[2]]), NROW(test[[2]])), twitter=c(NROW(train[[3]]), NROW(test[[3]])))
numSentences <- numSentences %>% mutate(total=c(sum(numSentences[1,2:4]), sum(numSentences[2,2:4])))
numSentences <- rbind(numSentences, c("total", 
                                    sum(numSentences$blogs), sum(numSentences$news), 
                                    sum(numSentences$twitter), sum(numSentences$total)))

numSentences$blogs <- as.numeric(numSentences$blogs)
numSentences$news <- as.numeric(numSentences$news)
numSentences$twitter <- as.numeric(numSentences$twitter)
numSentences$total <- as.numeric(numSentences$total)

numSentences
```

The training set holds roughly 70% of the tidy data, and the test set 30%.

## 2-Word Ngrams

```{r echo=FALSE}
plotNgrams <- function(n) {
  trainNgrams <- lapply(train, unnest_tokens, ngram, sentence, token="ngrams", n=n)
  trainNgrams <- lapply(trainNgrams, function(x) x[!is.na(x$ngram), ])
  trainCountN <- lapply(trainNgrams, count, ngram, sort=TRUE)
  
  trainTopN <- lapply(trainCountN, function(x) x[1:10, ])
  trainTopN <- lapply(filesIndex, function(x) {
    trainTopN[[x]] %>% mutate(file=dataFileNames[x], ngram=reorder(ngram, n))
  })
  
  p <- lapply(filesIndex, function(x) {
    ggplot(trainTopN[[x]], aes(x=ngram, y=n, fill=n)) + geom_col() + coord_flip() +
      xlab(NULL) + ylab(NULL) + ggtitle(dataNames[[x]]) + theme(legend.position="none")
  })
  
  combinedLevels <- union(levels(trainTopN[[1]]$ngram), levels(trainTopN[[2]]$ngram)) %>%
    union(levels(trainTopN[[3]]$ngram))
  trainTopN <- lapply(trainTopN, mutate, ngram=factor(ngram, levels=combinedLevels), 
                                                      file=NULL) %>% bind_rows()
  trainTopAll <- aggregate(n~ngram, trainTopN, sum) %>% mutate(ngram=reorder(ngram, n))

  left <- ggplot(trainTopAll, aes(x=ngram, y=n, fill=n)) + geom_col() + coord_flip() +
    xlab(NULL) + ylab(NULL) + ggtitle("all") + theme(legend.position="none")
  right <- ggarrange(p[[1]], p[[2]], p[[3]], ncol=1, nrow=3)
  
  ggarrange(left, right, ncol=2, nrow=1)
}
```

```{r echo=FALSE}
f <- plotNgrams(2)
f
## separate trigrams into words for searchability
# trainTrigrams %>% separate(col=trigram, into=c("first", "second", "third"))
## could also gsub first two words with blank and return the result
```

### 3-Word Ngrams

```{r echo=FALSE}
f <- plotNgrams(3)
f
```

* The blogs and twitter corpuses have ngrams with much higher frecuencies than news.

* Blogs and news share a few of the same ngrams.